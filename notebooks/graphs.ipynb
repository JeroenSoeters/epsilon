{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization over graphs\n",
    "\n",
    "This example is from [Hallac, Leskovec and Boyd, \"Network Lasso: Clustering and Optimization in Large Graphs\" (2015)](http://web.stanford.edu/~hallac/Network_Lasso.pdf) which considers a general class of optimization problems over a graphs (with vertices $\\mathcal{V}$ and edges $\\mathcal{E}$)\n",
    "$$\n",
    "\\DeclareMathOperator{\\minimize}{minimize} \\minimize \\;\\; \\sum_{i \\in \\mathcal{V}}f_i(x_i) + \\sum_{(j,k) \\in \\mathcal{E}} g_{jk}(x_j, x_k)\n",
    "$$\n",
    "where the optimization variable $x_i \\in \\mathbb{R}^p$ is associated with the graph vertex $i$. \n",
    "\n",
    "In particular, with each node we will associate a vector $a_i \\in \\mathbb{R}^{500}$ and solve the problem\n",
    "$$\n",
    "\\minimize \\;\\; \\sum_{i \\in \\mathcal{V}}\\|x_i - a_i\\|_2^2 + \\lambda \\sum_{(j,k) \\in \\mathcal{E}} \\|x_j - x_k\\|_2.\n",
    "$$\n",
    "Conceptually, each node would like to have its $x_i$ variable match $a_i$ but by regularizing the variables across the graph we encourage adjacent $x_j$, $x_k$ to be similar. The regularization penalty $\\|x_j - x_k\\|_2$ (which is referred to as \"sum-of-norms\" regularization or the \"group fused lasso\") will in actually create a clustering effect, encouraging many of the weights to be the *same* across neighbors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "import epopt as ep\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import snap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate a 3-regular random graph (every vertex has 3 neighbors) using [SNAP for Python](http://snap.stanford.edu/snappy/index.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate a random graph\n",
    "N = 2000\n",
    "K = 3\n",
    "graph = snap.GenRndDegK(N, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we write this problem in matrix form by introducing the differencing operator $D \\in \\{-1,0,1\\}^{|\\mathcal{E}| \\times |\\mathcal{N}|}$; for each edge between vertices $j$ and $k$, we add the following row to $D$:\n",
    "$$\n",
    "(0, \\ldots \\underset{\\substack{\\;\\;\\uparrow \\\\ \\;\\;j}}{-1},\n",
    "\\ldots \\underset{\\substack{\\uparrow \\\\ k}}{1}, \\ldots 0).\n",
    "$$\n",
    "This allows us to form the problem as\n",
    "$$\n",
    "\\minimize \\;\\; \\|X - A\\|_F^2 + \\|DX\\|_{2,1}\n",
    "$$\n",
    "with $X, A \\in \\mathbb{R}^{|\\mathcal{N}| \\times 500}$. Here $\\|\\cdot\\|_F$ denotes the Frobenius norm (the $\\ell_2$-norm applied to the elements of a matrix) and $\\|\\cdot\\|_{2,1}$ the $\\ell_2/\\ell_1$ mixed norm:\n",
    "$$\n",
    "\\|A\\|_{2,1} = \\sum_{i=1}^m \\left( \\sum_{j=1}^n A_{ij}^2 \\right)^{1/2}\n",
    "$$\n",
    "for $A \\in \\mathbb{R}^{m \\times n}$.\n",
    "\n",
    "In Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon 0.2.4\n",
      "Compiled prox-affine form:\n",
      "objective:\n",
      "  add(\n",
      "    sum_square(add(var(X), scalar(-1.00)*const(A))),\n",
      "    affine(dense(b)*var(y)),\n",
      "    second_order_cone(var(z), var(w)))\n",
      "\n",
      "constraints:\n",
      "  zero(add(kron(scalar(1.00), sparse(C))*var(X), scalar(-1.00)*var(w)))\n",
      "  zero(add(var(y), scalar(-1.00)*var(z)))\n",
      "Epsilon compile time: 0.0402 seconds\n",
      "\n",
      "iter=0 residuals primal=4.64e+02 [6.67e+00] dual=6.56e+02 [8.21e+00]\n",
      "iter=40 residuals primal=1.60e-02 [2.46e+01] dual=1.33e-01 [1.86e-01]\n",
      "Epsilon solve time: 42.9604 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('optimal', 94711.639709220675)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters\n",
    "N = 2000\n",
    "K = 3\n",
    "p = 500\n",
    "lam = 1\n",
    "\n",
    "# Generate random graph\n",
    "E = graph.GetEdges()\n",
    "\n",
    "# Construct differencing operator over graph\n",
    "data = np.hstack((np.ones(E), -np.ones(E)))\n",
    "i = np.hstack((np.arange(E), np.arange(E)))\n",
    "j = ([e.GetSrcNId() for e in graph.Edges()] +\n",
    "     [e.GetDstNId() for e in graph.Edges()])\n",
    "D = sp.coo_matrix((data, (i, j)))\n",
    "\n",
    "# Formulate problem\n",
    "X = cp.Variable(N, p)\n",
    "A = np.random.randn(N, p)\n",
    "f = cp.sum_squares(X-A) + lam*cp.sum_entries(cp.pnorm(D*X, 2, axis=1))\n",
    "prob = cp.Problem(cp.Minimize(f))\n",
    "\n",
    "# Solve with Epsilon\n",
    "ep.solve(prob, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we are able to solve this problem with 2000 x 500 = 1M variables regularized over a graph in about 40 seconds. Even more importantly, this graph-based optimization framework can easily be modified to incorporate many varieties of convex functions associated with nodes and edges to model many interesting problem, refer to [the full paper](http://web.stanford.edu/~hallac/Network_Lasso.pdf) for more examples.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

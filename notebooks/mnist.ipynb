{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST image classification\n",
    "\n",
    "In this example, we consider the classic machine learning dataset MNIST and the task of classifying handwritten digits. By modern computer vision standards this dataset is considered small, yet it is sufficiently large that many standard classifiers (e.g. those in the Python package `sklearn`) require significant time to train a model. Nonetheless, [Epsilon](http://epopt.io/) is able to fit a model that achieves near state-of-the-art accuracy in a few minutes. \n",
    "\n",
    "![MNIST examples](mnist.png)\n",
    "\n",
    "The standard task is to train a multiclass classifier that can correctly identify digits from their pixel intensity values. We will build a classifier to perform this task using [mutlticlass hinge loss](http://jmlr.csail.mit.edu/papers/volume2/crammer01a/crammer01a.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import io\n",
    "import urllib\n",
    "import cvxpy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "import epopt as ep\n",
    "\n",
    "\n",
    "data = \"http://epopt.s3.amazonaws.com/mnist.npz\"\n",
    "mnist = np.load(io.BytesIO(urllib.urlopen(data).read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass hinge loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multiclass hinge loss is a piecewise linear convex surrogate for the misclassification error in a multiclass problem; given a feature vector $x \\in \\mathbb{R}^n$ and label $y \\in \\{0,\\ldots,k\\}$ we incur loss\n",
    "$$\n",
    "\\max_j \\; \\{\\theta_j^Tx + 1 - \\delta_{j,y} \\} - \\theta_y^Tx\n",
    "$$\n",
    "\n",
    "where $\\theta_j \\in \\mathbb{R}^{n}$ is the weights for class $j$ and $\\delta_{p,q}$ is equal to $1$ if $p = q$ and $0$ otherwise. \n",
    "\n",
    "In order to minimize this function using CVXPY and Epsilon, we must write down its definition in matrix form. For convenience, Epsilon provides the `multiclass_hinge_loss()` function as well as several other common loss functions occuring in machine learning, see [functions.py](https://github.com/mwytock/epsilon/blob/master/python/epopt/functions.py) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def multiclass_hinge_loss(Theta, X, y):\n",
    "    k = Theta.size[1]\n",
    "    Y = one_hot(y, k)\n",
    "    return (cp.sum_entries(cp.max_entries(X*Theta + 1 - Y, axis=1)) -\n",
    "            cp.sum_entries(cp.mul_elemwise(X.T.dot(Y), Theta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also add a bit of $\\ell_2$-regularization on the parameter vectors $\\theta_1, \\ldots, \\theta_k$ to prevent over-fitting. The final optimization problem is\n",
    "$$\n",
    "\\DeclareMathOperator{\\minimize}{minimize} \\minimize \\;\\; \\sum_{i=1}^m \\left( \\max_j \\; \\{\\theta_j^Tx_i + 1 - \\delta_{j,y_i} \\} - \\theta_{y_i}^Tx_i \\right) + \\sum_{j=1}^k \\lambda \\|\\theta_j\\|_2^2\n",
    "$$\n",
    "\n",
    "where the parameter $\\lambda > 0$ controls the regularization. We set up the problem in CVXPY and solve with Epsilon as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon 0.2.4\n",
      "Compiled prox-affine form:\n",
      "objective:\n",
      "  add(\n",
      "    affine(dense(A)*var(x)),\n",
      "    non_negative(var(y)),\n",
      "    affine(kron(dense(B), dense(C))*diag(D)*var(Z)),\n",
      "    sum_square(var(W)))\n",
      "\n",
      "constraints:\n",
      "  zero(add(add(kron(transpose(dense(B)), scalar(1.00))*var(x), scalar(-1.00)*add(kron(scalar(1.00), dense(K))*var(W), dense(e)*1.00, scalar(-1.00)*const(F))), scalar(-1.00)*var(y)))\n",
      "  zero(add(var(Z), scalar(-1.00)*var(W)))\n",
      "Epsilon compile time: 1.4502 seconds\n",
      "\n",
      "iter=0 residuals primal=1.29e+05 [1.29e+03] dual=2.24e+02 [1.29e+03]\n",
      "iter=40 residuals primal=9.62e+00 [1.02e+01] dual=2.54e+01 [1.29e+03]\n",
      "Epsilon solve time: 38.7465 seconds\n",
      "Train error: 0.0853166666667\n",
      "Test error: 0.0891\n"
     ]
    }
   ],
   "source": [
    "# Problem data\n",
    "X = mnist[\"X\"] / 255.   # raw pixel data scaled to [0, 1]\n",
    "y = mnist[\"Y\"].ravel()  # labels {0, ..., 9}\n",
    "Xtest = mnist[\"Xtest\"] / 255.\n",
    "ytest = mnist[\"Ytest\"].ravel()\n",
    "\n",
    "# Parameters\n",
    "m, n = X.shape\n",
    "k = 10\n",
    "Theta = cp.Variable(n, k)\n",
    "lam = 1\n",
    "\n",
    "# Form problem with CVXPY and solve with Epsilon\n",
    "f = ep.multiclass_hinge_loss(Theta, X, y) + lam*cp.sum_squares(Theta)\n",
    "prob = cp.Problem(cp.Minimize(f))\n",
    "ep.solve(prob, verbose=True)\n",
    "\n",
    "# Get solution and compute train/test error\n",
    "def error(x, y):\n",
    "    return 1 - np.sum(x == y) / float(len(x))\n",
    "\n",
    "Theta0 = np.array(Theta.value)\n",
    "print \"Train error:\", error(np.argmax(X.dot(Theta0), axis=1), y)\n",
    "print \"Test error:\", error(np.argmax(Xtest.dot(Theta0), axis=1), ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, a simple linear classifier on pixel intensities achieves a 8.9% error rate on this task. This forms a reasonable baseline, but raw pixel values are in fact poor predictors and we can do much better by considering a nonlinear decision functions which we explore next. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear classifier using random Fourier features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out we can fit a non-linear decision function by approximating a Gaussian kernel using random Fourier features. In particular if we transform the input data by \n",
    "$$\n",
    "z(x) = \\cos(Wx + b)\n",
    "$$\n",
    "\n",
    "with $W \\in \\mathbb{R}^{d \\times n}$ with elements sampled from a zero-mean Normal distribution and $b \\in \\mathbb{R}^d$ with chosen uniformly at random from $[0, 2\\pi]$, then\n",
    "$$\n",
    "z(x)^Tz(x') \\approx \\exp \\left( \\frac{-\\|x - x'\\|_2^2}{2} \\right),\n",
    "$$\n",
    "\n",
    "for details see [Rahimi and Recht (2007)](http://www.eecs.berkeley.edu/~brecht/papers/07.rah.rec.nips.pdf). We will use this transformation to build a better classifier, with preprocessing following that of [Agarwal et al. (2014)](http://arxiv.org/abs/1310.1949), [code available here](https://github.com/fest/secondorderdemos). This is straightforward to implement in a few lines of Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def median_dist(X):\n",
    "    \"\"\"Compute the approximate median distance by sampling pairs.\"\"\"\n",
    "    k = 1<<20  # 1M random points\n",
    "    i = np.random.randint(0, X.shape[0], k)\n",
    "    j = np.random.randint(0, X.shape[0], k)\n",
    "    return np.sqrt(np.median(np.sum((X[i,:] - X[j,:])**2, axis=1)))\n",
    "    \n",
    "def pca(X, dim):\n",
    "    \"\"\"Perform centered PCA.\"\"\"\n",
    "    X = X - X.mean(axis=0)\n",
    "    return LA.eigh(X.T.dot(X))[1][:,-dim:]\n",
    "\n",
    "# PCA and median trick\n",
    "np.random.seed(0)\n",
    "V = pca(mnist[\"X\"], 50)\n",
    "X = mnist[\"X\"].dot(V)\n",
    "sigma = median_dist(X)\n",
    "\n",
    "# Random features\n",
    "n = 4000\n",
    "W = np.random.randn(X.shape[1], n) / sigma\n",
    "b = np.random.uniform(0, 2*np.pi, n)\n",
    "X = np.cos(X.dot(W) + b)\n",
    "Xtest = np.cos(mnist[\"Xtest\"].dot(V).dot(W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our transformed dataset we now have significantly more features (the feature matrix, $X \\in \\mathbb{R}^{60000 \\times 4000}$) but we still fit the model using the same method CVXPY/Epsilon and the same method as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon 0.2.4\n",
      "Compiled prox-affine form:\n",
      "objective:\n",
      "  add(\n",
      "    affine(dense(A)*var(x)),\n",
      "    non_negative(var(y)),\n",
      "    affine(kron(dense(B), dense(C))*diag(D)*var(Z)),\n",
      "    sum_square(var(W)))\n",
      "\n",
      "constraints:\n",
      "  zero(add(add(kron(transpose(dense(B)), scalar(1.00))*var(x), scalar(-1.00)*add(kron(scalar(1.00), dense(K))*var(W), dense(e)*1.00, scalar(-1.00)*const(F))), scalar(-1.00)*var(y)))\n",
      "  zero(add(var(Z), scalar(-1.00)*var(W)))\n",
      "Epsilon compile time: 9.8725 seconds\n",
      "\n",
      "iter=0 residuals primal=7.12e+05 [7.12e+03] dual=2.71e+02 [7.12e+03]\n",
      "iter=30 residuals primal=6.94e+00 [7.43e+00] dual=1.70e+01 [7.12e+03]\n",
      "Epsilon solve time: 196.5668 seconds\n",
      "Train error: 0.00501666666667\n",
      "Test error: 0.0157\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "m, n = X.shape\n",
    "k = 10\n",
    "Theta = cp.Variable(n, k)\n",
    "lam = 10\n",
    "\n",
    "# Form problem with CVXPY and solve with Epsilon\n",
    "f = ep.multiclass_hinge_loss(Theta, X, y) + lam*cp.sum_squares(Theta)\n",
    "prob = cp.Problem(cp.Minimize(f))\n",
    "ep.solve(prob, verbose=True)\n",
    "\n",
    "# Get solution and compute train/test error\n",
    "Theta0 = np.array(Theta.value)\n",
    "print \"Train error:\", error(np.argmax(X.dot(Theta0), axis=1), y)\n",
    "print \"Test error:\", error(np.argmax(Xtest.dot(Theta0), axis=1), ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our classifier now achieves an error rate of 1.57% improving significantly over the baseline. \n",
    "\n",
    "Critically, it only takes <3.5 minutes to train this classifier which is significantly faster than many of the dedicated Python machine learning packages (e.g. those provided by `sklearn`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "metadata": {
   "name": "Hello"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

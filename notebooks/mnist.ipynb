{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST \n",
    "\n",
    "In this example, we consider the classic machine learning dataset MNIST and the task of classifying handwritten digits. By modern computer vision standards this dataset is considered small, yet it is sufficiently large that many standard classifiers (e.g. those in the Python package `sklearn`) require significant time to train a model. Nonetheless, [Epsilon](http://epopt.io/) is able to fit a model that achieves near state-of-the-art accuracy in a few minutes. \n",
    "\n",
    "<img src=\"mnist.png\" />\n",
    "\n",
    "The standard task is to train a multiclass classifier that can correctly identify digits from their pixel intensity values. For the purposes of this example, we simplify this task slightly and instead consider the binary classification task of even vs. odd. To build our classifier we have a training set of 60K images of dimension 28x28 and a test set of 10K images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import io\n",
    "import urllib\n",
    "import cvxpy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "import epopt as ep\n",
    "\n",
    "mnist = np.load(io.BytesIO(urllib.urlopen(\"http://epopt.s3.amazonaws.com/mnist.npz\").read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hinge loss and support vector machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically we will set up our problem with a set of training examples $(X, y)$ where $X$ is a matrix containing $m$ training examples with $n$ input features ($X \\in \\mathbb{R}^{m \\times n}$) and $y \\in \\{-1,1\\}^m$ is the set of binary labels. Given this training set, we will fit the parameters of our model by minimizing some loss function\n",
    "$$\n",
    "\\DeclareMathOperator{\\minimize}{minimize} \\minimize \\;\\; \\ell(\\theta; X, y), \n",
    "$$\n",
    "where $\\theta \\in \\mathbb{R}^n$ denotes model parameters. A natural loss for classification tasks is the 0-1 loss: given a classifier, for each example we incur a penalty of 1 for each incorrect prediction and 0 otherwise. This loss function is nonconvex and so instead we minimize the *hinge loss*\n",
    "$$\n",
    "\\ell(\\theta; X, y) = \\sum_{i=1}^m \\max \\{0, 1 - y_i(\\theta^Tx_i) \\}.\n",
    "$$\n",
    "Conceptually, the hinge loss is in fact the tightest possible convex surrogate for the 0-1 loss as can be seen from the following picture:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAEACAYAAACuzv3DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHbBJREFUeJzt3X+01HW97/HnawQUBdkmLUhAOYk/Mr1kGpGmDOscW/gj\nSFanEG7da11zWV3trDr9OsGejevWyaPZ8thRtBLrCBvtUKlp2FKHH9XiHhNIAlL64VFQ9MaPQASF\n/b5/7JntMO69Z4Y9v+f1WGsvvjPzmfl+pk0v3n6+n8/nq4jAzMwaV6LWHTAzs4FxkJuZNTgHuZlZ\ng3OQm5k1OAe5mVmDc5CbmTW4ooJc0hGS1kh6oI/Xb5H0jKR1ks4ubxfNzKw/xVbk1wEbgDdNOpd0\nCTAhIk4BPgXcVr7umZlZIQWDXNJY4BLgu4B6aTIduBsgIlYDbZJGlbOTZmbWt2Iq8puBfwS6+nh9\nDPBczuPngbED7JeZmRWp3yCXdBnwUkSsofdqvKdp3mOv+zczq5JBBV4/D5ieGQc/CjhW0g8i4uM5\nbbYA43Iej808dwhJDnczs8MQEf0V0v1X5BHx1YgYFxF/A8wCHssLcYD7gY8DSJoM7IyIbb1+YAoS\nHQkWP7WYiGiqn/b29pr3wd/P383fr/l+ilHqPPLIBPbVkq7OhP1DwB8lbQYWAJ/u683tU9rpii7m\nLJ1D5/rOEk9tZma9KTS00iMilgPLM8cL8l77bDGfkUqmAOhY3sGcpXMAmHXmrGK7YGZmvSg6yMul\nWcM8mUzWugsV1czfr5m/G/j7tQIVOwYz4BNJkXuuVDpFx/IOEkpwz8x7miLMzczKTRJR4GJn1Svy\nrPzKXIiPnvnRWnXHrOlJ/WaB1YHDLaxrFuTQHeYRwfwV85m9dDaAw9ysgqr1X+BWuoH8Q1vTIIc3\nKnOHuZnZ4al5kEtymJuZDUDNgxwc5mZmA1E3N5bIhvm8C+fRFV3MXjqbJeuX1LpbZtYEFi5cyAUX\nXFDrblRM3QQ5OMzNWtn27du5/PLLGTZsGOPHj2fx4sX9tr/11ls599xzOeqoo7jyyiur1Mv6VBdD\nK7k8zGLWmj7zmc9w1FFH8dJLL7FmzRouvfRSJk6cyBlnnNFr+zFjxjB37lyWLVvGq6++WuXe1pe6\nqsizXJmbtZZXXnmFpUuXcv3113P00Udz/vnnM2PGDH74wx/2+Z7LL7+cGTNmcPzxx5d8vl/96le8\n5z3voa2tjUmTJvHrX/+657WFCxdy8sknc+yxx/L2t7+dRYsWAbB582amTJlCW1sbb33rW5k1q34W\nMdZdRZ7lytysusq5XqjU6epPP/00gwYNYsKECT3PTZw4kXQ6XcS5SjvZ9u3bufTSS7n11lu54oor\nuPfee7n00kv5wx/+wJAhQ7juuut44oknOOWUU9i2bRt/+ctfAJg7dy7Tpk1j+fLlvPbaazzxxBMl\nnbeS6rIiz3JlbtYa9uzZw7HHHnvIc8OHD2f37t0F31vqQpqf/exnnHbaacyZM4dEIsGsWbM4/fTT\nuf/++5FEIpHgqaee4tVXX2XUqFE9QztDhgzhz3/+M1u2bGHIkCGcd955JZ23kuo6yMFhblYtEeX7\nKdWwYcP461//eshzu3bt6gn3iy++mOHDhzN8+PA3XQQttSLfunUrJ5544iHPnXTSSWzdupWjjz6a\nJUuWcPvtt3PCCSdw2WWX8fvf/x6AG264gYhg0qRJnHnmmdx1112lfs2KqfsgB4e5WbM79dRTOXDg\nAJs3b+55bt26dbzzne8E4OGHH2b37t3s3r2bK6644pD3llqRjxkzhmefffaQ55599lnGjBkDwAc+\n8AEeeeQRXnzxRU4//XSuuuoqAEaNGsUdd9zBli1bWLBgAZ/+9Kf54x//WPJ3rYSGCHJwmJs1s2OO\nOYaZM2cyb9489u7dy6pVq3jggQf42Mc+1ud7Dh48yL59+zhw4AAHDx5k//79HDx4sOC5Lr74Yp5+\n+mkWL17MgQMHWLJkCZs2beKyyy7jpZde4qc//SmvvPIKgwcP5phjjuGII44A4L777uP5558HoK2t\nrWcYpi5U8XZFUQ5dXV0x77F5QYpIdCSi86nOsnyuWbMr1/8HK2X79u3xoQ99KI455pg46aSTYvHi\nxf22b29vD0mH/HR0dPTaduHChXHBBRf0PF61alWcc845MWLEiDj33HPjl7/8ZUREvPDCCzFlypQY\nMWJEtLW1xdSpU2Pjxo0REfHFL34xxowZE8OGDYuTTz457rzzzjJ98259/X4yz/ebrzXbj3wgIoJU\nOsX8FfNJKMGimYs8m8WsgMy+1rXuhvWhr99PXe9HPhCemmhm9oaCAzySjpK0WtJaSRskfaOXNklJ\nuyStyfx8rTLdPeScHjM3M6OIijwi9kmaGhF7JQ0CVkl6f0Ssymu6PCKmV6abvXNlbmZW5NBKROzN\nHA4BjgC299KsJveRcpibWasrau6MpISktcA24PGI2JDXJIDzJK2T9JCk3ne5qRAPs5hZKysqyCOi\nKyLeBYwFLpSUzGvyJDAuIiYC/wr8pKy9LILD3MxaVUmzViJil6SfAecC6Zznd+ccPyzp3yS9JSIO\nGYJJpVI9x8lkkmQyeXi97oOHWcys0aXT6aI2C8tVcB65pJHAgYjYKWkosAzoiIhHc9qMAl6KiJA0\nCbg3IsbnfU7Z5pEXkj/P/J6Z9zDrzPrZctKsFjyPvL4NZB55MUMrbwMey4yRrwYeiIhHJV0t6epM\nmw8DT2XafBuoaWpKomNqB+1T2umKLuYsnUPn+s5adsnMChg/fjyPPvpor6+tXLmS008/vco9apxb\nxBUz/fAp4N29PL8g5/g7wHfK27WByw6zdCzvYM7SOQCuzM3qlKQ+N8C64IIL2LRpU5V71DgacmVn\nKRzmZtbs6mTrrspKJVOHDLN4NotZfVqzZg0TJ06kra2NWbNmsX//fqD7AuC4ceN62o0fP56bbrqp\n17bQvXf4CSecwNixY/nud79LIpHo2XJ2//79fOELX+Ckk05i9OjRXHPNNezbt6+o/tXrLeKaviLP\nSiVTRIRns5j1QR3lW9MX7aVfVI0I7rvvPpYtW8aRRx7J+eefz8KFC7n66qvf1FZSn21//vOfc/PN\nN/PYY48xfvz4nv3Es7785S/zpz/9iXXr1jFo0CBmz57N/Pnz+frXv95v/+r5FnEtUZFneZ65Wf2S\nxLXXXsvo0aM57rjj+OAHP8jatWv7bN9X23vvvZdPfOITvOMd72Do0KF0dHT0vCciuPPOO/nWt75F\nW1sbw4YN4ytf+QqdnYUnQ9TzLeJapiIHzzM368/hVNHlNnr06J7joUOHsnXr1qLbvvDCCwC88MIL\nTJo0qee1sWPH9hy//PLL7N27l3POOafnuYigq6urYN+KuUXcjTfeyCc/+UnOP/98brrpJk477TRu\nuOEG5s6dy6RJkzjuuOP4/Oc/z5VXXlnwfKVoqYocvALUrNm97W1v47nnnut5nHs8cuRIhg4dyoYN\nG9ixYwc7duxg586db7pfaG/q+RZxLRfk8OZ55g5zs8aXXUzzkY98hLvuuotNmzaxd+9err/++p42\niUSCq666is997nO8/PLLAGzZsoVHHnmk4OfX8y3iWjLIs3JnszjMzepL/rzy/m6ynNt22rRpXHvt\ntUydOpVTTz2V973vfQAceeSRAHzzm99kwoQJTJ48mREjRnDRRRfx9NNPF/zc448/ngcffJCbbrqJ\nkSNHcuONN/Lggw/ylre8ha6uLm6++WbGjBnD8ccfz8qVK7ntttsAeOKJJ5g8eTLDhw9nxowZ3HLL\nLYwfP37A//sc0s9GvNVbuaXSKTqWd/i2cdbUWnWJ/saNGznrrLN47bXX6udmyb2o9BL9pufK3Ky5\n/PjHP2b//v3s2LGDL33pS0yfPr2uQ3ygmveblSg/zL03i1njuuOOOxg1ahQTJkxg8ODBPcMczcpD\nK3lyh1m8a6I1k1YdWmkUAxlaaal55MXw3ixm1mgc5L1wmJtZI3GQ98FhbmaNwkHej/wwF/LURGto\n/c3FtsblIC/AuyZas/CFzublIC+CN9oys3rmIC+Cd000s3rW74IgSUdJWi1praQNkr7RR7tbJD0j\naZ2ksyvT1dryrolmVq/6rcgjYp+kqRGxV9IgYJWk90fEqmwbSZcAEyLiFEnvBW4DJle227XhytzM\n6lHBJfoRsTdzOAQ4Atie12Q6cHem7WqgTdKocnaynrgyN7N6UzDIJSUkrQW2AY9HxIa8JmOA53Ie\nPw+MpYn1Fubem8XMaqXgxc6I6ALeJWkEsExSMiLSec3yJ6f2Os8plUr1HCeTSZLJZCl9rSv5wyxe\nNGRm5ZBOp0mn0yW9p6RNsyTNBV6NiBtznrsdSEdEZ+bxJmBKRGzLe29DbJpVqogglU4xf8V8b7Rl\nZmU34P3IJY2U1JY5HgpcBKzJa3Y/8PFMm8nAzvwQb2bZynzuhXPpii7mLJ3jYRYzq6pCQytvA+6W\nlKA79H8YEY9KuhogIhZExEOSLpG0GXgFKO/toRuAJDqSHQBcv+J6D7OYWVV5P/Iy8jCLmZWb9yOv\nst4ugHqjLTOrNAd5mXnRkJlVm4O8AhzmZlZNDvIKcZibWbU4yCvIYW5m1eAgrzCHuZlVmoO8Chzm\nZlZJDvIqcZibWaU4yKvIYW5mleAgr7LewjwIrwA1s8PmIK8Bb4FrZuXkIK8RSXRM7d5oy2FuZgPh\nIK8xV+ZmNlAO8hrzMIuZDZSDvA5410QzGwgHeZ3w1EQzO1wO8jriMDezw+EgrzMOczMrlYO8DjnM\nzawUiUINJI2T9Lik30laL+naXtokJe2StCbz87XKdLd1ZMN83oXz6IouZi+dzZL1S2rdLTOrQ8VU\n5K8D/xARayUNA34j6RcRsTGv3fKImF7+LrYuL+c3s2IUrMgj4sWIWJs53gNsBE7opWm/d3m2w5Nf\nmc9ZOofO9Z217paZ1ZGCQZ5L0njgbGB13ksBnCdpnaSHJJ1Rnu4ZOMzNrH9FX+zMDKv8CLguU5nn\nehIYFxF7JV0M/AQ4Nf8zUqlUz3EymSSZTB5Gl1uTV4CatYZ0Ok06nS7pPYqIwo2kwcCDwMMR8e0i\n2v8JOCcituc8F8WcywpLpVN0LO8goQSLZi7ybBazJiaJiOh36LqYWSsCvgds6CvEJY3KtEPSJLr/\ngdjeW1sbuFQyRfuUds9mMTOguKGV84H/DvxW0prMc18FTgSIiAXAh4FrJB0A9gL+7/0Kyw6zdCzv\n8DxzsxZX1NBKWU7koZWK8DCLWXMrZmjFKzsbnCtzM3OQNwGHuVlrc5A3CYe5WetykDcRh7lZa3KQ\nNxmHuVnrcZA3IYe5WWtxkDcph7lZ63CQNzGHuVlrcJA3ufww937mZs3HQd4CcsPcuyaaNR8HeYtw\nmJs1Lwd5C3GYmzUnB3mLyQ9zIV8ANWtwDvIW5NksZs3FQd6iHOZmzcNB3sIc5mbNwUHe4hzmZo3P\nQW4Oc7MG5yA3wGFu1sgShRpIGifpcUm/k7Re0rV9tLtF0jOS1kk6u/xdtUpLJVO0T2mnK7qYvXQ2\nnes7a90lMytCMRX568A/RMRaScOA30j6RURszDaQdAkwISJOkfRe4DZgcmW6bJXkRUNmjadgRR4R\nL0bE2szxHmAjcEJes+nA3Zk2q4E2SaPK3FerktzKfM7SOa7MzepcwSDPJWk8cDawOu+lMcBzOY+f\nB8YOpGNWWw5zs8ZR9MXOzLDKj4DrMpX5m5rkPY78BqlUquc4mUySTCaLPb3VgIdZzKovnU6TTqdL\neo8i3pS3b24kDQYeBB6OiG/38vrtQDoiOjOPNwFTImJbTpso5lxWf1LpFB3LO0gowT0z73GYm1WR\nJCIiv1A+RDGzVgR8D9jQW4hn3A98PNN+MrAzN8StsXmYxay+FazIJb0fWAH8ljeGS74KnAgQEQsy\n7W4FpgGvAFdGxJN5n+OKvMHlVuaLZi7yPHOzKiimIi9qaKVMnXGQN4H2x9uZv2K+w9ysSooJcq/s\ntJJkL4DOXzHfK0DN6oSD3EoiyWFuVmcc5FYyh7lZfXGQ22FxmJvVDwe5HbbewjwIzzM3qzIHuQ1I\nfph7BahZ9TnIbcAk0TG1o/tPL+c3qzoHuZWN92Yxqw0HuZWVw9ys+hzkVnYOc7PqcpBbRTjMzarH\nQW4V4zA3q46S7hBkVqpUMsW8C+f1bIG7ZP2SWnfJrOm4IreK8wpQs8pykFvFeTm/WWU5yK0qHOZm\nleMgt6pxmJtVhoPcqsphblZ+DnKrOu+aaFZeBacfSvq+pG2Snurj9aSkXZLWZH6+Vv5uWrPJbrTV\nPqW9Z2pi5/rOWnfLrCEVU5HfBfwr8IN+2iyPiOnl6ZK1Ei8aMhu4ghV5RKwEdhRo1u8dns36k0qm\nXJmbDUA5VnYGcJ6kdZIeknRGGT7TWkz+ClCHuVnxynGx80lgXETslXQx8BPg1N4aplKpnuNkMkky\nmSzD6a1Z+E5DZpBOp0mn0yW9RxFRuJE0HnggIs4qou2fgHMiYnve81HMuay1RQSpdIr5K+aTUIJ7\nZt7jMLeWJomI6Hf4esBDK5JGSVLmeBLd/zhsL/A2s15lpyZ6oy2z4hUcWpG0GJgCjJT0HNAODAaI\niAXAh4FrJB0A9gIun2xAvGjIrDRFDa2U5UQeWrES5Q+zLJq5yGFuLaeYoRWv7LS65crcrDgOcqtr\nDnOzwhzkVvcc5mb9c5BbQ/BGW2Z9c5Bbw8gPcy8aMuvmILeGkt01UZI32jLLcJBbQ/KuiWZvcJBb\nw3KYm3VzkFtDSyVTRITHzK2lOcit4fkCqLU6B7k1PM9msVbnILem0FuYC3nRkLUEB7k1Da8AtVbl\nILem4jC3VuQgt6bjMLdW4yC3puQwt1biILem5TC3VuEgt6bmXROtFTjIrel5oy1rdolCDSR9X9I2\nSU/10+YWSc9IWifp7PJ20aw8UskU7VPa6You5iydQ+f6zlp3yawsCgY5cBcwra8XJV0CTIiIU4BP\nAbeVqW9mZecwt2ZUMMgjYiWwo58m04G7M21XA22SRpWne2bllx/mS9YvqXWXzAakHGPkY4Dnch4/\nD4wFtuU3lMpwNrOySMHUoGuKZ7NY4yvXxc78iI7em6VyjpOZH7MaeTzFSSfCs3/jMLf6kU6nSafT\nJb1HEX1kbm4jaTzwQESc1ctrtwPpiOjMPN4ETImIbXntophzmVXDmjXw7nfDf5sYfOjmFPNXzCeh\nBItmLnKYW12RRET0O55RzMXOQu4HPp454WRgZ36Im9WbtrbuP/+6q3ue+bwL59EVXcxeOttj5tZw\nCg6tSFoMTAFGSnoOaAcGA0TEgoh4SNIlkjYDrwBXVrLDZuUwYkT3nzt3egWoNb6ihlbKciIPrVgd\nOXAABg+GRAJef737z4gglfYwi9WXag2tmDWcQYNg2DDo6oI9e7qfy1bmHmaxRuMgt5aVHSffteuN\n5xzm1ogc5NaycsfJc/UW5l4BavXMm2ZZy8pW5PlBDr6hszUWB7m1rGxFnju0ksu7JlqjcJBby+qv\nIs+Vrcwd5lavHOTWsgpV5Lkc5lbPfLHTWlaxFXlW7gVQ75po9cQVubWsUiryLK8AtXrkILeWVWpF\nDl7Ob/XJQW4t63AqcnCYW/1xkFvLOpyKPMthbvXEQW4t63Ar8iyHudULB7m1rIFU5FkOc6sHDnJr\nWQOtyLMc5lZrDnJrWeWoyLMc5lZLXhBkLWvo0O59yfftg/37B/553jXRasVBbi1L6n1P8oF9ZvdG\nW+1T2ntWgDrMrdKKCnJJ0yRtkvSMpC/18npS0i5JazI/Xyt/V83Kr1zj5PlSyZTD3KqmmJsvHwHc\nCvwdsAX4T0n3R8TGvKbLI2J6BfpoVjHlHCfP5422rFqKqcgnAZsj4s8R8TrQCczopV2/Nwc1q0eV\nqsiz8itzb7RllVBMkI8Bnst5/HzmuVwBnCdpnaSHJJ1Rrg6aVVIlK/Is3wPUKq2Y6YdRRJsngXER\nsVfSxcBPgFMH1DOzKqh0RZ7lqYlWScUE+RZgXM7jcXRX5T0iYnfO8cOS/k3SWyJie267VCrVc5xM\nJkkmk4fRZbPyqUZFDp5nbsVLp9Ok0+mS3qOI/gtuSYOA3wN/C2wF/i9wRe7FTkmjgJciIiRNAu6N\niPF5nxOFzmVWbR0dkErB3Lkwf37lzxcRpNIp5q+YT0IJFs1c5DC3fkkiIvq9BlmwIo+IA5I+CywD\njgC+FxEbJV2deX0B8GHgGkkHgL2AL81bQ6hWRZ7lytwqoagl+hHxMPBw3nMLco6/A3ynvF0zq7xq\njZHncphbuXmvFWtp1a7IsxzmVk4OcmtptajIsxzmVi4OcmtptarIs3oL8yC8AtRK4iC3llbLijwr\nP8y9nN9K5SC3llbrijwru2uiJO/NYiUrOI+8bCfyPHKrQwcOwODBkEjA6693/1lrqXSKjuUdJJTg\nnpn3OMxbXDHzyOvgr61Z7QwaBMOGQVcX7NlT69508xa4VioHubW8ehgnz5e70ZZ3TbRCHOTW8upl\nnDyfd020Yvlip7W8ct/urVw8z9yK5SC3lpcdWqm3ihwc5lYcB7m1vHqtyLMc5laIg9xaXj1X5FkO\nc+uPg9xaXr1X5FkOc+uLg9xaXiNU5FkOc+uNg9xaXqNU5FkOc8vnILeW10gVeZZ3TbRcDnJrefW6\nIKgQb7RlWQ5ya3n1uES/FNnK3GHeugou0Zc0TdImSc9I+lIfbW7JvL5O0tnl76ZZ5TRqRZ7LG221\ntn6DXNIRwK3ANOAM4ApJ78hrcwkwISJOAT4F3Fahvta1dDpd6y5UVDN/v9/9Lg00bkWelR/m2b1Z\nmvl3B83//YpRqCKfBGyOiD9HxOtAJzAjr8104G6AiFgNtEkaVfae1rlm/8vUzN9vzZo00NgVeVZu\nmGc32mrm3x0099/NYhUaIx8DPJfz+HngvUW0GQtsG3DvzKpg0KDun337YPXq7uNG9sHhKbaeAnc+\n08Hs/5jN+RsvYcKjv6l1tyrmt3/cyr838fcrRqG/ssXe0if/7hW+FZA1DAmOOw5efhkmT651b8ol\nBUnoSnawctuDrFz1YK07VDn/BT9edWete1FT/d7qTdJkIBUR0zKPvwJ0RcQ3c9rcDqQjojPzeBMw\nJSK25X2Ww93M7DAUutVboYr8CeAUSeOBrcBHgSvy2twPfBbozAT/zvwQL6YjZmZ2ePoN8og4IOmz\nwDLgCOB7EbFR0tWZ1xdExEOSLpG0GXgFuLLivTYzsx79Dq2YmVn9q+o9OyVdn1k0tFbSo5LGVfP8\nlSbpXyRtzHzHpZJG1LpP5SLp7yX9TtJBSe+udX/KpZgFb41K0vclbZP0VK37UgmSxkl6PPP3cr2k\na2vdp3KRdJSk1Zms3CDpG/22r2ZFLml4ROzOHP9vYGJE/K+qdaDCJF0EPBoRXZL+GSAivlzjbpWF\npNOBLmAB8PmIeLLGXRqwzIK33wN/B2wB/hO4IiI21rRjZSLpAmAP8IOIOKvW/Sk3SaOB0RGxVtIw\n4DfAh5ro93d0ROyVNAhYBXwhIlb11raqFXk2xDOGAf+vmuevtIj4RUR0ZR6upns+fVOIiE0R8XSt\n+1FmxSx4a1gRsRLYUet+VEpEvBgRazPHe4CNwAm17VX5RMTezOEQuq9Rbu+rbVWDHEDS/5H0X8D/\nAP652uevok8AD9W6E9av3hazjalRX2wAMjPrzqa7gGoKkhKS1tK9uPLxiNjQV9uyr2GT9AtgdC8v\nfTUiHoiIfwL+SdKXgZtpsFkuhb5fps0/Aa9FxKKqdm6AivluTcZX+ptAZljlR8B1mcq8KWT+6/5d\nmWttyyQlIyLdW9uyB3lEXFRk00U0YMVa6PtJ+p/AJcDfVqVDZVTC765ZbAFyL7iPo7sqtwYhaTDw\nH8C/R8RPat2fSoiIXZJ+BpwLpHtrU+1ZK6fkPJwBrKnm+StN0jTgH4EZEbGv1v2poGZZ3NWz4E3S\nELoXvN1f4z5ZkSQJ+B6wISK+Xev+lJOkkZLaMsdDgYvoJy+rPWvlR8BpwEHgD8A1EfFS1TpQYZKe\nofvCRPaixK8j4tM17FLZSLocuAUYCewC1kTExbXt1cBJuhj4Nm8seOt3mlcjkbQYmAIcD7wEzIuI\nu2rbq/KR9H5gBfBb3hgm+0pE/Lx2vSoPSWfRvatsIvPzw4j4lz7be0GQmVljq/qsFTMzKy8HuZlZ\ng3OQm5k1OAe5mVmDc5CbmTU4B7mZWYNzkJuZNTgHuZlZg/v/Q+P2dSOs6tQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f69738891d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-3,3, 100)\n",
    "plt.plot(z, z < 0, linewidth=2)\n",
    "plt.plot(z, np.maximum(0, 1 - z), linewidth=2)\n",
    "_ = plt.legend((\"0-1 loss\", \"hinge loss\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to minimize this function using CVXPY and Epsilon, we must write down its definition which is a single line of Python. For convenience, Epsilon provides the `hinge_loss()` function as well as several others, see [`functions.py`](https://github.com/mwytock/epsilon/blob/master/python/epopt/functions.py) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hinge_loss(theta, X, y):\n",
    "    return cp.sum_entries(cp.max_elemwise(1 - sp.diags([y],[0])*X*theta, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define the parameters formulate the optimization problem and solve. We also add a bit of $\\ell_2$-regularization on the parameter vector $\\theta$ to keep the values from getting too large. The final optimization problem is\n",
    "$$\n",
    "\\minimize \\;\\; \\ell(\\theta; X, y) + \\lambda \\|\\theta\\|_2^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon 0.2.4\n",
      "Compiled prox-affine form:\n",
      "objective:\n",
      "  add(\n",
      "    sum_hinge(var(x)),\n",
      "    sum_square(var(y)))\n",
      "\n",
      "constraints:\n",
      "  zero(add(add(dense(a)*1.00, scalar(-1.00)*dense(B)*var(y)), scalar(-1.00)*var(x)))\n",
      "Epsilon compile time: 1.3997 seconds\n",
      "\n",
      "iter=0 residuals primal=1.52e+02 [2.47e+00] dual=1.92e+02 [1.55e+00]\n",
      "iter=70 residuals primal=8.04e-01 [6.23e+00] dual=1.13e+00 [1.28e+00]\n",
      "Epsilon solve time: 18.1032 seconds\n",
      "Train error: 0.0961833333333\n",
      "Test error: 0.0979\n"
     ]
    }
   ],
   "source": [
    "# Problem data\n",
    "X = mnist[\"X\"] / 255. \n",
    "y = (mnist[\"Y\"].ravel() % 2 == 0)*2-1  # convert labels to {-1,1}\n",
    "Xtest = mnist[\"Xtest\"] / 255.\n",
    "ytest = (mnist[\"Ytest\"].ravel() % 2 == 0)*2-1\n",
    "\n",
    "# Parameters\n",
    "m, n = X.shape\n",
    "theta = cp.Variable(n)\n",
    "lam = 1\n",
    "\n",
    "# Form problem with CVXPY and solve with Epsilon\n",
    "f = ep.hinge_loss(theta, X, y) + lam*cp.sum_squares(theta)\n",
    "prob = cp.Problem(cp.Minimize(f))\n",
    "ep.solve(prob, verbose=True)\n",
    "\n",
    "# Get solution and evaluate 0-1 error\n",
    "def error(x, y):    \n",
    "    return 1 - np.sum(x == y) / float(len(x))\n",
    "\n",
    "theta0 = np.ravel(theta.value)\n",
    "print \"Train error:\", error((X.dot(theta0)>0)*2-1, y)\n",
    "print \"Test error:\", error((Xtest.dot(theta0)>0)*2-1, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, a simple linear classifier on pixel intensities achieves a 9.8% error rate on this task. This forms a reasonable baseline, but raw pixel values are in fact poor predictors and we can do much better by considering a nonlinear decision functions which we explore next. \n",
    "\n",
    "First, its worth noting that the hinge loss also naturally arises in the derivation of  [support vector machines](https://en.wikipedia.org/wiki/Support_vector_machine). In addition, SVMs allow efficient learning of non-linear classification boundaries via their dual formulation and the \"kernel trick\". In fact, SVMs with Gaussian kernel  \n",
    "$$ k(x, x') = \\exp\\left( \\frac{-\\|x - x'\\|^2}{2\\sigma^2}  \\right) $$\n",
    "are known to perform very well on MNIST.\n",
    "\n",
    "Unfortunately as the size of the training set is 60K, explicitly instantiating the kernel matrix (60K x 60K) is prohibitively expensive for this problem. Instead, we will use a recent method based on random Fourier features which approximates the kernel distance function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear classifier using random Fourier features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather remarkably, it turns out that transforming the input data points as\n",
    "$$\n",
    "\n",
    "$$\n",
    "for details see Rahimi and Recht, Random Features for Large-scale Kernel Machi\n",
    "\n",
    "The paritulcar application of this idea to the MNIST dataset is based loosely on the implementation from [this paper](least squares revisited) which has code available [here](github.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def median_dist(X):\n",
    "    \"\"\"Compute the approximate median distance by sampling pairs.\"\"\"\n",
    "    k = 1<<20\n",
    "    i = np.random.randint(0, Xp.shape[0], k)\n",
    "    j = np.random.randint(0, Xp.shape[0], k)\n",
    "    return np.sqrt(np.median(np.sum((Xp[i,:] - Xp[j,:])**2, axis=1)))\n",
    "    \n",
    "def pca(X, dim):\n",
    "    \"\"\"Perform centered PCA.\"\"\"\n",
    "    X = X - X.mean(axis=0)\n",
    "    return LA.eigh(X.T.dot(X))[1][:,-dim:]\n",
    "\n",
    "# PCA and median trick\n",
    "np.random.seed(0)\n",
    "V = pca(mnist[\"X\"], 50)\n",
    "X = mnist[\"X\"].dot(V)\n",
    "sigma = median_dist(X)\n",
    "\n",
    "# Random features\n",
    "n = 4000\n",
    "W = np.random.randn(Xp.shape[1], n) / sigma\n",
    "b = np.random.uniform(0, 2*np.pi, n)\n",
    "X = np.cos(X.dot(W) + b)\n",
    "Xtest = np.cos(mnist[\"Xtest\"].dot(V).dot(W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our transformed dataset, the next step is to fit the classifier. We apply the `hinge_loss()` function discussed above along with some regularization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon 0.2.4\n",
      "Compiled prox-affine form:\n",
      "objective:\n",
      "  add(\n",
      "    sum_hinge(var(x)),\n",
      "    sum_square(var(y)))\n",
      "\n",
      "constraints:\n",
      "  zero(add(add(dense(a)*1.00, scalar(-1.00)*dense(B)*var(y)), scalar(-1.00)*var(x)))\n",
      "Epsilon compile time: 9.6309 seconds\n",
      "\n",
      "iter=0 residuals primal=7.23e+01 [2.47e+00] dual=2.33e+02 [1.07e+00]\n",
      "iter=80 residuals primal=4.63e-01 [6.14e+00] dual=1.54e+00 [1.59e+00]\n",
      "Epsilon solve time: 136.8507 seconds\n",
      "Train error: 0.00685\n",
      "Test error: 0.0129\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "m, n = X.shape\n",
    "theta = cp.Variable(n)\n",
    "lam = 10\n",
    "\n",
    "# Form problem with CVXPY and solve with Epsilon\n",
    "f = ep.hinge_loss(theta, X, y) + lam*cp.sum_squares(theta)\n",
    "prob = cp.Problem(cp.Minimize(f))\n",
    "ep.solve(prob, verbose=True)\n",
    "\n",
    "# Get solution\n",
    "theta0 = np.ravel(theta.value)\n",
    "print \"Train error:\", error((X.dot(theta0)>0)*2-1, y)\n",
    "print \"Test error:\", error((Xtest.dot(theta0)>0)*2-1, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

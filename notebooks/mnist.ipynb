{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST \n",
    "\n",
    "In this example, we consider the classic machine learning dataset MNIST and the task of classifying handwritten digits. By modern computer vision standards this dataset is considered small, yet it is sufficiently large that many standard classifiers (e.g. those in the Python package `sklearn`) require significant time to train a model. Nonetheless, [Epsilon](http://epopt.io/) is able to fit a model that achieves near state-of-the-art accuracy in a few minutes. \n",
    "\n",
    "<img src=\"mnist.png\" />\n",
    "\n",
    "The standard task is to train a multiclass classifier that can correctly identify digits from their pixel intensity values. For the purposes of this example, we simplify this task slightly and instead consider the binary classification task of even vs. odd. To build our classifier we have a training set of 60K images of dimension 28x28 and a test set of 10K images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import io\n",
    "import urllib\n",
    "import cvxpy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "import epopt as ep\n",
    "\n",
    "mnist = np.load(io.BytesIO(urllib.urlopen(\"http://epopt.s3.amazonaws.com/mnist.npz\").read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hinge loss and support vector machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically we will set up our problem with a set of training examples $(X, y)$ where $y \\in \\{-1,1\\}^m$ is the set of binary labels and $X$ is a matrix containing $m$ training examples with $n$ input features (the features for each, $x_i \\in \\mathbb{R}^n$, are arranged as the rows of matrix $X \\in \\mathbb{R}^{m \\times n}$). Given this training set, we will fit the parameters of our model by minimizing a loss function over the training data\n",
    "$$\n",
    "\\DeclareMathOperator{\\minimize}{minimize} \\minimize \\;\\; \\ell(\\theta; X, y), \n",
    "$$\n",
    "where $\\theta \\in \\mathbb{R}^n$ denotes model parameters. A natural loss for classification is the 0-1 loss: we incur a penalty of 1 for each incorrect classification and 0 otherwise. This loss function is nonconvex and so instead we minimize a convex surrogate, the *hinge loss*\n",
    "$$\n",
    "\\ell(\\theta; X, y) = \\sum_{i=1}^m \\max \\{0, 1 - y_i(\\theta^Tx_i) \\}.\n",
    "$$\n",
    "The hinge loss is in fact the tightest possible convex surrogate for the 0-1 loss as can be seen from the following picture:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEACAYAAACj0I2EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwoFuZlYnHOhmZnXi/wN0X7sgFOEXkwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x108e1b910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# z = y*p is the product of the label and the prediction, i.e. z>0 is a correct prediction\n",
    "z = np.linspace(-3, 3, 1000)\n",
    "plt.plot(z, z < 0, linewidth=2)\n",
    "plt.plot(z, np.maximum(0, 1 - z), linewidth=2)\n",
    "_ = plt.legend((\"0-1 loss\", \"hinge loss\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to minimize this function using CVXPY and Epsilon, we must write down its definition which is a single line of Python. For convenience, Epsilon provides the `hinge_loss()` function as well as several others, see [`functions.py`](https://github.com/mwytock/epsilon/blob/master/python/epopt/functions.py) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hinge_loss(theta, X, y):\n",
    "    return cp.sum_entries(cp.max_elemwise(1 - sp.diags([y],[0])*X*theta, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also add a bit of $\\ell_2$-regularization on the parameter vector $\\theta$, the final optimization problem is\n",
    "$$\n",
    "\\minimize \\;\\; \\ell(\\theta; X, y) + \\lambda \\|\\theta\\|_2^2\n",
    "$$\n",
    "where the parameter $\\lambda > 0$ controls the amount of regularization (and therefore, the bias-variance tradeoff between over/underfitting the data). Formulating this problem in CVXPY and solving it with Epsilon is done as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon 0.2.4\n",
      "Compiled prox-affine form:\n",
      "objective:\n",
      "  add(\n",
      "    sum_hinge(var(x)),\n",
      "    sum_square(var(y)))\n",
      "\n",
      "constraints:\n",
      "  zero(add(add(dense(a)*1.00, scalar(-1.00)*dense(B)*var(y)), scalar(-1.00)*var(x)))\n",
      "Epsilon compile time: 3.1065 seconds\n",
      "\n",
      "iter=0 residuals primal=1.52e+02 [2.47e+00] dual=1.92e+02 [1.55e+00]\n",
      "iter=70 residuals primal=8.04e-01 [6.23e+00] dual=1.13e+00 [1.28e+00]\n",
      "Epsilon solve time: 24.0632 seconds\n",
      "Train error: 0.0961833333333\n",
      "Test error: 0.0979\n"
     ]
    }
   ],
   "source": [
    "# Problem data\n",
    "X = mnist[\"X\"] / 255.                  # raw pixel data scaled to [0, 1]\n",
    "y = (mnist[\"Y\"].ravel() % 2 == 0)*2-1  # labels converted to {-1,1}\n",
    "Xtest = mnist[\"Xtest\"] / 255.\n",
    "ytest = (mnist[\"Ytest\"].ravel() % 2 == 0)*2-1\n",
    "\n",
    "# Parameters\n",
    "m, n = X.shape\n",
    "theta = cp.Variable(n)\n",
    "lam = 1\n",
    "\n",
    "# Form problem with CVXPY and solve with Epsilon\n",
    "f = ep.hinge_loss(theta, X, y) + lam*cp.sum_squares(theta)\n",
    "prob = cp.Problem(cp.Minimize(f))\n",
    "ep.solve(prob, verbose=True)\n",
    "\n",
    "# Get solution and evaluate 0-1 error\n",
    "def error(x, y):    \n",
    "    return 1 - np.sum(x == y) / float(len(x))\n",
    "\n",
    "theta0 = np.ravel(theta.value)\n",
    "print \"Train error:\", error((X.dot(theta0)>0)*2-1, y)\n",
    "print \"Test error:\", error((Xtest.dot(theta0)>0)*2-1, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, a simple linear classifier on pixel intensities achieves a 9.8% error rate on this task. This forms a reasonable baseline, but raw pixel values are in fact poor predictors and we can do much better by considering a nonlinear decision functions which we explore next. \n",
    "\n",
    "First, its worth noting that the hinge loss also naturally arises in the derivation of  [support vector machines](https://en.wikipedia.org/wiki/Support_vector_machine). In addition, SVMs allow efficient learning of non-linear classification boundaries via their dual formulation and the \"kernel trick\". In fact, SVMs with Gaussian kernel  \n",
    "$$ k(x, x') = \\exp\\left( \\frac{-\\|x - x'\\|^2}{2\\sigma^2}  \\right) $$\n",
    "are known to perform very well on MNIST.\n",
    "\n",
    "Unfortunately as the size of the training set is 60K, explicitly instantiating the kernel matrix (60K x 60K) is prohibitively expensive for this problem. Instead, we will use a recent method based on random Fourier features which approximates the kernel distance function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear classifier using random Fourier features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather remarkably, it turns out that transforming the input data with random features\n",
    "$$\n",
    "z(x) = \\cos(Wx + b)\n",
    "$$\n",
    "with $W \\in \\mathbb{R}^{d \\times n}$ and $b \\in \\mathbb{R}^d$; The elements of $W$ are sampled from a zero-mean Normal distribution $\\mathcal{N}(0, \\sigma^2)$ and the elements of $b$ are chosen uniformly at random from $[0, 2\\pi]$. When the input features are transformed in this fashion, their inner product approximates the Gaussian kernel\n",
    "$$\n",
    "z(x)^Tz(x') \\approx k(x, x'),\n",
    "$$\n",
    "for the complete mathematical details see [Rahimi and Recht (2007)](http://www.eecs.berkeley.edu/~brecht/papers/07.rah.rec.nips.pdf).\n",
    "\n",
    "Our implementation of this idea (e.g. the use of PCA, median trick) is inspired by a [recent paper](http://arxiv.org/abs/1310.1949) ([code](https://github.com/fest/secondorderdemos)) which reports near state-of-the-art performance on the MNIST digit classification class. \n",
    "\n",
    "These transformations are straightforward to implement in a few lines of Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def median_dist(X):\n",
    "    \"\"\"Compute the approximate median distance by sampling pairs.\"\"\"\n",
    "    k = 1<<20  # 1M random points\n",
    "    i = np.random.randint(0, Xp.shape[0], k)\n",
    "    j = np.random.randint(0, Xp.shape[0], k)\n",
    "    return np.sqrt(np.median(np.sum((Xp[i,:] - Xp[j,:])**2, axis=1)))\n",
    "    \n",
    "def pca(X, dim):\n",
    "    \"\"\"Perform centered PCA.\"\"\"\n",
    "    X = X - X.mean(axis=0)\n",
    "    return LA.eigh(X.T.dot(X))[1][:,-dim:]\n",
    "\n",
    "# PCA and median trick\n",
    "np.random.seed(0)\n",
    "V = pca(mnist[\"X\"], 50)\n",
    "X = mnist[\"X\"].dot(V)\n",
    "sigma = median_dist(X)\n",
    "\n",
    "# Random features\n",
    "n = 4000\n",
    "W = np.random.randn(Xp.shape[1], n) / sigma\n",
    "b = np.random.uniform(0, 2*np.pi, n)\n",
    "X = np.cos(X.dot(W) + b)\n",
    "Xtest = np.cos(mnist[\"Xtest\"].dot(V).dot(W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our transformed dataset we now have a larger feature matrix ($X \\in \\mathbb{R}^{60000 \\times 4000}$) but we fit the model using the same method consisting of hinge loss and $\\ell_2$-regularization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon 0.2.4\n",
      "Compiled prox-affine form:\n",
      "objective:\n",
      "  add(\n",
      "    sum_hinge(var(x)),\n",
      "    sum_square(var(y)))\n",
      "\n",
      "constraints:\n",
      "  zero(add(add(dense(a)*1.00, scalar(-1.00)*dense(B)*var(y)), scalar(-1.00)*var(x)))\n",
      "Epsilon compile time: 9.6309 seconds\n",
      "\n",
      "iter=0 residuals primal=7.23e+01 [2.47e+00] dual=2.33e+02 [1.07e+00]\n",
      "iter=80 residuals primal=4.63e-01 [6.14e+00] dual=1.54e+00 [1.59e+00]\n",
      "Epsilon solve time: 136.8507 seconds\n",
      "Train error: 0.00685\n",
      "Test error: 0.0129\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "m, n = X.shape\n",
    "theta = cp.Variable(n)\n",
    "lam = 10\n",
    "\n",
    "# Form problem with CVXPY and solve with Epsilon\n",
    "f = ep.hinge_loss(theta, X, y) + lam*cp.sum_squares(theta)\n",
    "prob = cp.Problem(cp.Minimize(f))\n",
    "ep.solve(prob, verbose=True)\n",
    "\n",
    "# Get solution\n",
    "theta0 = np.ravel(theta.value)\n",
    "print \"Train error:\", error((X.dot(theta0)>0)*2-1, y)\n",
    "print \"Test error:\", error((Xtest.dot(theta0)>0)*2-1, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our classifier now achieves an error rate of 1.3% improving significantly over the baseline. The current version of Epsilon (0.2.4) is able to fit the parameters of this model in <2.5 minutes which is itself significantly faster than several of the dedicated classifier implementations available in standard packages (e.g. Python's `sklearn`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example provides a quick introduction to achieving very good results on the classical machine learning task MNIST. [Epsilon](http://epopt.io/) along with the environment provided by [CVXPY](http://cvxpy.org) and [NumPy](http://numpy.org) make it very fast to iterate on this classifier, e.g. we could expeirment with different random features, add $\\ell_1$-regularization instead of or in addition to the $\\ell_2$-regularization, perform more careful cross-validation, etc...\n",
    "\n",
    "In addition to these refinements on this application, there are several items on the Epsilon roadmap which are relevant here\n",
    "- proximal operators for multiclass classification (e.g. multiclass hinge loss, softmax, etc.) - would allow us to efficiently solve the standard MNIST problem directly \n",
    "- warm start - would allow for more efficient cross-validation by caching work across nearly identical problem instances \n",
    "- multicore support - splitting the data matrix by row would likely improve convergence speed significantly\n",
    "- GPU support - in this problem the main bottleneck is dense linear algebra---this could be much faster with a GPU implementation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

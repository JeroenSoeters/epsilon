{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST image classification\n",
    "\n",
    "In this example, we consider the classic machine learning dataset MNIST and the task of classifying handwritten digits. By modern computer vision standards this dataset is considered small, yet it is sufficiently large that many standard classifiers (e.g. those in the Python package `sklearn`) require significant time to train a model. Nonetheless, [Epsilon](http://epopt.io/) is able to fit a model that achieves near state-of-the-art accuracy in a few minutes. \n",
    "\n",
    "<img src=\"mnist.png\" />\n",
    "\n",
    "The standard task is to train a multiclass classifier that can correctly identify digits from their pixel intensity values. We will build a classifier to perform this task using [mutlticlass hinge loss](http://jmlr.csail.mit.edu/papers/volume2/crammer01a/crammer01a.pdf) which is straightforward to express in CVXPY form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import io\n",
    "import urllib\n",
    "import cvxpy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "import epopt as ep\n",
    "\n",
    "mnist = np.load(io.BytesIO(urllib.urlopen(\"http://epopt.s3.amazonaws.com/mnist.npz\").read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hinge loss, multiclass classification and support vector machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically we will set up our problem with a set of training examples $(X, y)$ where $y \\in \\{-1,1\\}^m$ is the set of binary labels and $X$ is a matrix containing $m$ training examples with $n$ input features (the features for each, $x_i \\in \\mathbb{R}^n$, are arranged as the rows of matrix $X \\in \\mathbb{R}^{m \\times n}$). Given this training set, we will fit the parameters of our model by minimizing a loss function over the training data\n",
    "$$\n",
    "\\DeclareMathOperator{\\minimize}{minimize} \\minimize \\;\\; \\ell(\\theta; X, y), \n",
    "$$\n",
    "where $\\theta \\in \\mathbb{R}^n$ denotes model parameters. A natural loss for classification is the 0-1 loss: we incur a penalty of 1 for each incorrect classification and 0 otherwise. This loss function is nonconvex and so instead we minimize a convex surrogate, the *hinge loss*\n",
    "$$\n",
    "\\ell(\\theta; X, y) = \\sum_{i=1}^m \\max \\{0, 1 - y_i(\\theta^Tx_i) \\}.\n",
    "$$\n",
    "The hinge loss is in fact the tightest possible convex surrogate for the 0-1 loss as can be seen from the following picture:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEACAYAAACj0I2EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwoFuZlYnHOhmZnXi/wN0X7sgFOEXkwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x108e1b910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# z = y*p is the product of the label and the prediction, i.e. z>0 is a correct prediction\n",
    "z = np.linspace(-3, 3, 1000)\n",
    "plt.plot(z, z < 0, linewidth=2)\n",
    "plt.plot(z, np.maximum(0, 1 - z), linewidth=2)\n",
    "_ = plt.legend((\"0-1 loss\", \"hinge loss\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multiclass generalization of hinge loss extends this idea to predicting $k$ classess..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to minimize this function using CVXPY and Epsilon, we must write down its definition which is a single line of Python. For convenience, Epsilon provides the `multiclass_hinge_loss()` function as well as several others, see [`functions.py`](https://github.com/mwytock/epsilon/blob/master/python/epopt/functions.py) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def multiclass_hinge_loss(Theta, X, y):\n",
    "    k = Theta.size[1]\n",
    "    Y = one_hot(y, k)\n",
    "    return (cp.sum_entries(cp.max_entries(X*Theta + 1 - Y, axis=1)) -\n",
    "            cp.sum_entries(cp.mul_elemwise(X.T.dot(Y), Theta)))\n",
    "\n",
    "def error(x, y):\n",
    "    return 1 - np.sum(x == y) / float(len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also add a bit of $\\ell_2$-regularization on the parameter vector $\\theta$, the final optimization problem is\n",
    "$$\n",
    "\\minimize \\;\\; \\ell(\\theta; X, y) + \\lambda \\|\\theta\\|_2^2\n",
    "$$\n",
    "where the parameter $\\lambda > 0$ controls the amount of regularization (and therefore, the bias-variance tradeoff between over/underfitting the data). Formulating this problem in CVXPY and solving it with Epsilon is done as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon 0.2.4\n",
      "Compiled prox-affine form:\n",
      "objective:\n",
      "  add(\n",
      "    affine(dense(A)*var(x)),\n",
      "    non_negative(var(y)),\n",
      "    affine(kron(dense(B), dense(C))*diag(D)*var(Z)),\n",
      "    sum_square(var(W)))\n",
      "\n",
      "constraints:\n",
      "  zero(add(add(kron(transpose(dense(B)), scalar(1.00))*var(x), scalar(-1.00)*add(kron(scalar(1.00), dense(K))*var(W), dense(e)*1.00, scalar(-1.00)*const(F))), scalar(-1.00)*var(y)))\n",
      "  zero(add(var(Z), scalar(-1.00)*var(W)))\n",
      "Epsilon compile time: 1.4502 seconds\n",
      "\n",
      "iter=0 residuals primal=1.29e+05 [1.29e+03] dual=2.24e+02 [1.29e+03]\n",
      "iter=40 residuals primal=9.62e+00 [1.02e+01] dual=2.54e+01 [1.29e+03]\n",
      "Epsilon solve time: 38.7465 seconds\n",
      "Train error: 0.0853166666667\n",
      "Test error: 0.0891\n"
     ]
    }
   ],
   "source": [
    "# Problem data\n",
    "X = mnist[\"X\"] / 255.   # raw pixel data scaled to [0, 1]\n",
    "y = mnist[\"Y\"].ravel()  # labels {0, ..., 9}\n",
    "Xtest = mnist[\"Xtest\"] / 255.\n",
    "ytest = mnist[\"Ytest\"].ravel()\n",
    "\n",
    "# Parameters\n",
    "m, n = X.shape\n",
    "k = 10\n",
    "Theta = cp.Variable(n, k)\n",
    "lam = 1\n",
    "\n",
    "# Form problem with CVXPY and solve with Epsilon\n",
    "f = ep.multiclass_hinge_loss(Theta, X, y) + lam*cp.sum_squares(Theta)\n",
    "prob = cp.Problem(cp.Minimize(f))\n",
    "ep.solve(prob, verbose=True)\n",
    "\n",
    "# Get solution and compute train/test error\n",
    "Theta0 = np.array(Theta.value)\n",
    "print \"Train error:\", error(np.argmax(X.dot(Theta0), axis=1), y)\n",
    "print \"Test error:\", error(np.argmax(Xtest.dot(Theta0), axis=1), ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, a simple linear classifier on pixel intensities achieves a 8.9% error rate on this task. This forms a reasonable baseline, but raw pixel values are in fact poor predictors and we can do much better by considering a nonlinear decision functions which we explore next. \n",
    "\n",
    "First, its worth noting that the hinge loss also naturally arises in the derivation of  [support vector machines](https://en.wikipedia.org/wiki/Support_vector_machine). In addition, SVMs allow efficient learning of non-linear classification boundaries via their dual formulation and the \"kernel trick\". In fact, SVMs with Gaussian kernel  \n",
    "$$ k(x, x') = \\exp\\left( \\frac{-\\|x - x'\\|^2}{2\\sigma^2}  \\right) $$\n",
    "are known to perform very well on MNIST.\n",
    "\n",
    "Unfortunately as the size of the training set is 60K, explicitly instantiating the kernel matrix (60K x 60K) is prohibitively expensive for this problem. Instead, we will use a recent method based on random Fourier features which approximates the kernel distance function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear classifier using random Fourier features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather remarkably, it turns out that transforming the input data with random features\n",
    "$$\n",
    "z(x) = \\cos(Wx + b)\n",
    "$$\n",
    "with $W \\in \\mathbb{R}^{d \\times n}$ and $b \\in \\mathbb{R}^d$; The elements of $W$ are sampled from a zero-mean Normal distribution $\\mathcal{N}(0, \\sigma^2)$ and the elements of $b$ are chosen uniformly at random from $[0, 2\\pi]$. When the input features are transformed in this fashion, their inner product approximates the Gaussian kernel\n",
    "$$\n",
    "z(x)^Tz(x') \\approx k(x, x'),\n",
    "$$\n",
    "for the complete mathematical details see [Rahimi and Recht (2007)](http://www.eecs.berkeley.edu/~brecht/papers/07.rah.rec.nips.pdf).\n",
    "\n",
    "Our implementation of this idea (e.g. the use of PCA, median trick) is inspired by a [recent paper](http://arxiv.org/abs/1310.1949) ([code](https://github.com/fest/secondorderdemos)) which reports near state-of-the-art performance on the MNIST digit classification class. \n",
    "\n",
    "These transformations are straightforward to implement in a few lines of Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def median_dist(X):\n",
    "    \"\"\"Compute the approximate median distance by sampling pairs.\"\"\"\n",
    "    k = 1<<20  # 1M random points\n",
    "    i = np.random.randint(0, X.shape[0], k)\n",
    "    j = np.random.randint(0, X.shape[0], k)\n",
    "    return np.sqrt(np.median(np.sum((X[i,:] - X[j,:])**2, axis=1)))\n",
    "    \n",
    "def pca(X, dim):\n",
    "    \"\"\"Perform centered PCA.\"\"\"\n",
    "    X = X - X.mean(axis=0)\n",
    "    return LA.eigh(X.T.dot(X))[1][:,-dim:]\n",
    "\n",
    "# PCA and median trick\n",
    "np.random.seed(0)\n",
    "V = pca(mnist[\"X\"], 50)\n",
    "X = mnist[\"X\"].dot(V)\n",
    "sigma = median_dist(X)\n",
    "\n",
    "# Random features\n",
    "n = 4000\n",
    "W = np.random.randn(X.shape[1], n) / sigma\n",
    "b = np.random.uniform(0, 2*np.pi, n)\n",
    "X = np.cos(X.dot(W) + b)\n",
    "Xtest = np.cos(mnist[\"Xtest\"].dot(V).dot(W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our transformed dataset we now have a larger feature matrix ($X \\in \\mathbb{R}^{60000 \\times 4000}$) but we fit the model using the same method consisting of hinge loss and $\\ell_2$-regularization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon 0.2.4\n",
      "Compiled prox-affine form:\n",
      "objective:\n",
      "  add(\n",
      "    affine(dense(A)*var(x)),\n",
      "    non_negative(var(y)),\n",
      "    affine(kron(dense(B), dense(C))*diag(D)*var(Z)),\n",
      "    sum_square(var(W)))\n",
      "\n",
      "constraints:\n",
      "  zero(add(add(kron(transpose(dense(B)), scalar(1.00))*var(x), scalar(-1.00)*add(kron(scalar(1.00), dense(K))*var(W), dense(e)*1.00, scalar(-1.00)*const(F))), scalar(-1.00)*var(y)))\n",
      "  zero(add(var(Z), scalar(-1.00)*var(W)))\n",
      "Epsilon compile time: 9.8725 seconds\n",
      "\n",
      "iter=0 residuals primal=7.12e+05 [7.12e+03] dual=2.71e+02 [7.12e+03]\n",
      "iter=30 residuals primal=6.94e+00 [7.43e+00] dual=1.70e+01 [7.12e+03]\n",
      "Epsilon solve time: 196.5668 seconds\n",
      "Train error: 0.00501666666667\n",
      "Test error: 0.0157\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "m, n = X.shape\n",
    "k = 10\n",
    "Theta = cp.Variable(n, k)\n",
    "lam = 10\n",
    "\n",
    "# Form problem with CVXPY and solve with Epsilon\n",
    "f = ep.multiclass_hinge_loss(Theta, X, y) + lam*cp.sum_squares(Theta)\n",
    "prob = cp.Problem(cp.Minimize(f))\n",
    "ep.solve(prob, verbose=True)\n",
    "\n",
    "# Get solution and compute train/test error\n",
    "Theta0 = np.array(Theta.value)\n",
    "print \"Train error:\", error(np.argmax(X.dot(Theta0), axis=1), y)\n",
    "print \"Test error:\", error(np.argmax(Xtest.dot(Theta0), axis=1), ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our classifier now achieves an error rate of 1.57% improving significantly over the baseline. Critically, it only 3.5 minutes to train this classifier which is significantly faster than many of the dedicated Python machine learning packages (e.g. those provided by `sklearn`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example provides a quick introduction to achieving very good results on the classical machine learning task MNIST using [Epsilon](http://epopt.io/). Its important to note that this environment provided by [CVXPY](http://cvxpy.org) and [NumPy](http://numpy.org) other numerical Python packages makes it very fast to iterate on this classifier, e.g. we could expeirment with different random features, add $\\ell_1$-regularization instead of or in addition to the $\\ell_2$-regularization, perform more careful cross-validation, etc...\n",
    "\n",
    "In addition, there are several items on the Epsilon roadmap which are relevant:\n",
    "- multicore support - splitting the data matrix by row would likely improve convergence speed significantly\n",
    "- GPU support - in this problem the main bottleneck is dense linear algebra---this could be much faster with a GPU implementation\n",
    "- warm start - would allow for more efficient cross-validation by caching work across nearly identical problem instances "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
